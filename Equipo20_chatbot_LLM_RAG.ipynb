{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-hVND8xY2OKY"
   },
   "source": [
    "# **Procesamiento de Lenguaje Natural**\n",
    "\n",
    "## Maestr√≠a en Inteligencia Artificial Aplicada\n",
    "#### Tecnol√≥gico de Monterrey\n",
    "#### Prof Luis Eduardo Falc√≥n Morales\n",
    "\n",
    "### **Adtividad en Equipos: sistema LLM + RAG**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aimHVFOv23lm"
   },
   "source": [
    "* **Nombres y matr√≠culas:**\n",
    "\n",
    "* üßë‚Äçüíª Ovidio Alejandro Hern√°ndez Ruano (A01796714)\n",
    "* üßë‚Äçüíª Jos√© Manuel Toral Cruz (A01122243)\n",
    "* üßë‚Äçüíª Oscar Enrique Garc√≠a Garc√≠a (A01016093)\n",
    "* üßë‚Äçüíª Luis Gerardo Sanchez Salazar (A01232963)\n",
    "\n",
    "* **N√∫mero de Equipo:**\n",
    "* Equipo #20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7jimvsiVgjMg"
   },
   "source": [
    "* ##### **El formato de este cuaderno de Jupyter es libre, pero debe incuir al menos las siguientes secciones:**\n",
    "\n",
    "  * ##### **Introducci√≥n de la problem√°tica a resolver.**\n",
    "  * ##### **Sistema RAG + LLM**\n",
    "  * ##### **El chatbot, incluyendo ejemplos de prueba.**\n",
    "  * ##### **Conclusiones**\n",
    "\n",
    "* ##### **Pueden importar los paquetes o librer√≠as que requieran.**\n",
    "\n",
    "* ##### **Pueden incluir las celdas y l√≠neas de c√≥digo que deseen.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2akzn895aF_"
   },
   "source": [
    "# **Introducci√≥n de la problem√°tica a resolver.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fq3bI2BBBm97"
   },
   "source": [
    "## üí° Ansiedad y depresi√≥n: Falta de acceso inmediato, confiable y seguro a informaci√≥n\n",
    "\n",
    "**La ansiedad y la depresi√≥n** son dos de los transtornos de salud mental m√°s comunes a nivel mundial, afectando a millones de personas de todas las edades. A pesar de su prevalencia, muchas personas no cuentan con acceso oportuno a informaci√≥n clara, comprensible y basada en evidencia que les permita entender sus s√≠ntomas, buscar ayuda o simplemente sentirse acompa√±adas en su proceso.\n",
    "\n",
    "Factores como la estigma social, la desinformaci√≥n, las barreras econ√≥micas, la escasez de profesionales en salud mental y largos tiempos de espera para recibir atenci√≥n dificultan quee quienes sufren de estos trastornos obtengan orientaci√≥n adecuada. Adem√°s, en entornos digitales sobrecargados de contenido, es frecuente encontrar informaci√≥n err√≥nea, poco √∫til o incluso perjudicial.\n",
    "\n",
    "En este contexto, se propone el desarrollo de un **chatbot** especializado en ansiedad y depresi√≥n, que brinde respuestas confiables, emp√°ticas y accesibles. Este asistente digital tiene como objetivo reducir la brecha de informaci√≥n, orientar a las personas hacia recursos apropiados y contribuir al bienestar emocional desde un enfoque preventivo y educativo.\n",
    "\n",
    "Para el desarrollo del chatbot, se usar√° Retrieval-Augmented Generation (**RAG**) en combinaci√≥n de modelos de lenguaje de gran escala (**LLM**), generando as√≠ una soluci√≥n efectiva, principalmente por las siguientes razones:\n",
    "\n",
    "* ‚úÖ **Actualizaci√≥n y precisi√≥n sobre el contenido:** RAG permite que el modelo no genere respuestas aleatorias basadas en su entrenamiento previo, sino que consulte una base documental curada como gu√≠as cl√≠nicas, material psicoeducativo validado, publicaciones cient√≠ficas, o preguntas frecuentes verificadas, reduciendo el riesgo de respuestas incorrectas o no fundamentadas, lo cual es crucial al tratar temas sensibles como la salud mental.\n",
    "\n",
    "* ‚úÖ **Actualizaci√≥n din√°mica y adaptabilidad**: A diferencia de un LLM puro, que no puede aprender nueva informaci√≥n tras su entrenamiento, un sistema RAG puede actualizar f√°cilmente su corpus de conocimiento sin necesidad de reentrenamiento. As√≠, el chatbot puede mantenerse actualizado con las √∫ltimas recomendaciones cl√≠nicas, normativas locales o recursos disponibles (como l√≠neas de ayuda o centros de atenci√≥n comunitarios) o incluso, diagn√≥sticos, conversaciones, o historiales cl√≠nicos del paciente para una experiencia m√°s personalizada.\n",
    "\n",
    "* ‚úÖ **Respuestas contextuales, emp√°ticas y comprensibles**: El LLM permite que las respuestas se generen en lenguaje natural, adaptadas al tono del usuario, con un estilo conversacional emp√°tico y accesible. Esto ayuda a que las personas se sientan comprendidas y no juzgadas, y que la informaci√≥n se transmita con claridad y calidez.\n",
    "\n",
    "* ‚úÖ **Escalabilidad y disponibilidad**: Al ser una soluci√≥n automatizada, el chatbot puede estar disponible 24/7, sin importar la zona horaria ni la demanda, lo que permite llegar a m√°s personas en momentos de urgencia o duda. Esto lo convierte en una herramienta valiosa en comunidades con recursos limitados.\n",
    "\n",
    "* ‚úÖ **Privacidad y anonimato**: Muchos usuarios prefieren explorar sus dudas sobre salud mental de forma an√≥nima, sin tener que hablar cara a cara con un profesional desde el inicio. Un chatbot con estas caracter√≠sticas ofrece un espacio seguro, privado y no intrusivo donde dar los primeros pasos hacia el autocuidado o la b√∫squeda de ayuda.\n",
    "\n",
    "Combinar RAG + LLM para desarrollar un chatbot sobre ansiedad y depresi√≥n no solo mejora la calidad y seguridad de las respuestas, sino que adem√°s ofrece una soluci√≥n t√©cnica √©tica, escalable y centrada en el bienestar del usuario. Esta herramienta no pretende reemplazar el acompa√±amiento profesional, sino funcionar como una gu√≠a accesible, informada y confiable que ayude a reducir barreras, brindar contenci√≥n y facilitar el acceso a recursos de salud mental adecuados.\n",
    "\n",
    "A continuaci√≥n se presenta una propuesta desarrollada con **fines exclusivamente educativos**. Por ello, las respuestas generadas deben ser interpretadas como orientaciones generales y no deben considerarse asesoramiento m√©dico, diagn√≥stico cl√≠nico ni sustituto de atenci√≥n profesional. Se recomienda que los usuarios consulten a especialistas en salud mental ante cualquier duda o situaci√≥n que requiera intervenci√≥n profesional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQL0S7wX54Vh"
   },
   "source": [
    "# **Paquetes y librer√≠as**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmeyvDTjBGHV"
   },
   "source": [
    "## üîç Procesamiento de Lenguaje Natural (NLP)\n",
    "* **üìö transformers:**  Biblioteca de modelos preentrenados desarrollada por Hugging Face. Permite usar modelos de NLP como BERT, GPT, T5, etc., para tareas como clasificaci√≥n, resumen, traducci√≥n, y m√°s.\n",
    "\n",
    "* **üß† sentence-transformers:** Extensi√≥n sobre transformers enfocada en generar representaciones vectoriales (embeddings) de oraciones y p√°rrafos. Ideal para tareas como b√∫squeda sem√°ntica, comparaci√≥n de textos y clustering.\n",
    "\n",
    "* **üìä datasets:** Colecci√≥n de datasets estandarizados de Hugging Face para NLP. Simplifica la carga, preprocesamiento y uso de datos para entrenamiento y evaluaci√≥n de modelos.\n",
    "\n",
    "## üì¶ Vectorizaci√≥n y B√∫squeda Sem√°ntica\n",
    "* **üß≠ faiss-cpu:** Biblioteca de b√∫squeda eficiente de vectores desarrollada por Facebook AI. Se usa para encontrar r√°pidamente los elementos m√°s cercanos en grandes conjuntos de embeddings.\n",
    "\n",
    "## üñºÔ∏è Interfaz de Usuario / Aplicaciones Web\n",
    "* **üß™ gradio:**\n",
    " Herramienta para crear interfaces web r√°pidas y simples para modelos de machine learning. Ideal para prototipar y compartir modelos con inputs y outputs interactivos (texto, audio, imagen, etc.).\n",
    "\n",
    "## üìÑ Lectura de Documentos PDF\n",
    "* **üìÑ PyMuPDF:** Biblioteca para leer, extraer texto y trabajar con archivos PDF y otros documentos. Muy √∫til para convertir PDFs a texto para an√°lisis posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UVMo8aNh595N",
    "outputId": "c448ab88-41b7-4878-a869-e5700b7f72a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.52.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.6.15)\n",
      "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
      "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\n",
      "Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\n",
      "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.33.0)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
      "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\n",
      "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
      "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.13)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (1.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.6.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.11/dist-packages (1.26.1)\n"
     ]
    }
   ],
   "source": [
    "# Incluyan a continuaci√≥n todas las celdas (de c√≥digo o texto) que deseen...\n",
    "!pip install transformers\n",
    "!pip install sentence-transformers\n",
    "!pip install faiss-cpu\n",
    "!pip install gradio\n",
    "!pip install datasets\n",
    "!pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJtWHl5iBMpo"
   },
   "source": [
    "## üß± Utilidades del Sistema y Datos\n",
    "\n",
    "* **üìÇ os:** M√≥dulo est√°ndar de Python para interactuar con el sistema operativo. Se usa para operaciones como navegar entre carpetas, crear directorios o manipular archivos.\n",
    "\n",
    "* **üì¶ pickle:** Biblioteca est√°ndar de Python para serializar y deserializar objetos. √ötil para guardar modelos, datos procesados o estructuras en archivos binarios para reutilizaci√≥n.\n",
    "\n",
    "* **üî¢ numpy:** Paquete fundamental para computaci√≥n num√©rica en Python. Permite manejar arrays y realizar operaciones matem√°ticas eficientes a nivel de matriz.\n",
    "\n",
    "* **üåé IPython.display:** Importamos las funciones `display` y `HTML` para desplegar contenido enriquecido como HTML dentro de celdas de Google Colab.\n",
    "\n",
    "* **üî° re:** M√≥dulo de expresiones regulares en Python. Se usa para buscar, reemplazar o dividir texto mediante patrones definidos.\n",
    "\n",
    "## üìÑ Procesamiento de Documentos PDF\n",
    "\n",
    "* **üìÑ fitz (PyMuPDF):** Interfaz principal de la biblioteca PyMuPDF. Permite abrir, leer y extraer texto o im√°genes de archivos PDF y otros documentos.\n",
    "\n",
    "## üì¶ B√∫squeda Sem√°ntica\n",
    "\n",
    "* **üß≠ faiss:** Biblioteca de Facebook AI para b√∫squeda r√°pida de vectores en grandes vol√∫menes de datos. Se usa para tareas de similitud sem√°ntica y recuperaci√≥n de informaci√≥n basada en embeddings.\n",
    "\n",
    "## üîç Procesamiento de Lenguaje Natural (NLP)\n",
    "\n",
    "* **üß† sentence-transformers:** Biblioteca que permite generar embeddings sem√°nticos de textos. Generalmente utilizada para tareas como clustering, recuperaci√≥n sem√°ntica y comparaci√≥n de oraciones.\n",
    "\n",
    "* **üß¨ transformers (AutoTokenizer, AutoModelForSeq2SeqLM, pipeline):** Componentes clave de la librer√≠a Hugging Face Transformers:\n",
    "  - **AutoTokenizer:** Convierte texto a tokens entendibles por modelos.\n",
    "  - **AutoModelForSeq2SeqLM:** Carga modelos preentrenados para tareas de secuencia a secuencia.\n",
    "  - **pipeline:** Interfaz simplificada para aplicar tareas comunes con modelos preentrenados.\n",
    "\n",
    "* **üó£Ô∏è nltk & sent_tokenize:** `nltk` es una biblioteca para procesamiento de lenguaje natural. En este caso, se utiliza para **tokenizar texto en oraciones** mediante `sent_tokenize`, √∫til para dividir grandes textos en unidades m√°s manejables.\n",
    "\n",
    "## üìê Normalizaci√≥n de Vectores\n",
    "\n",
    "* **üìê normalize:** Se utiliza para escalar vectores de forma que tengan una norma unitaria. Es esencial para calcular similitudes de forma coherente entre vectores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X2KsDfQ0BUh_",
    "outputId": "0364cf68-b1a3-49b1-e6c1-45764988233f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz\n",
    "import faiss\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from IPython.display import display, HTML\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tthhwfJhBo0o"
   },
   "source": [
    "## üìÇ Archivos PDF y sus descripciones\n",
    "\n",
    "A continuaci√≥n se enlistan los archivos a utilizar para el chatbot, junto con su tema principal y una berve descripci√≥n de su conrtenido.\n",
    "\n",
    "**1. empatia_comunicacion.pdf**\n",
    "* Tema: Empat√≠a en la comunicaci√≥n\n",
    "* Descripci√≥n: Describe el concepto de empat√≠a desde el punto de vista cognitivo y emocional, destacando la importancia de las neuronas espejo y c√≥mo la empat√≠a mejora la calidad de la comunicaci√≥n interpersonal. Incluye t√©cnicas como la escucha activa, actitud de apertura y rechazo de prejuicios\n",
    "\n",
    "\n",
    "**2. primeras-paginas-lo-mejor-de-tu-vida-es.pdf**\n",
    "* Tema: Autoestima y confianza personal\n",
    "* Descripci√≥n: Fragmento del libro de Ma. Jes√∫s √Ålava Reyes. Enfatiza la importancia de confiar en uno mismo para alcanzar la felicidad. Incluye reflexiones, testimonios y cap√≠tulos orientados al desarrollo de la autoconfianza y la superaci√≥n del sufrimiento emocional\n",
    "\n",
    "\n",
    "**3. Guia de Primeros Auxilios Psicol√≥gicos_Integra.pdf**\n",
    "* Tema: Primeros Auxilios Psicol√≥gicos (PAP)\n",
    "* Descripci√≥n: Gu√≠a orientada a personas en movilidad humana. Explica qu√© es una crisis psicol√≥gica, c√≥mo se manifiesta, tipos de intervenci√≥n, principios del PAP, y autocuidado del personal interviniente. Incluye t√©cnicas como escucha emp√°tica, estabilizaci√≥n y contenci√≥n emocional\n",
    "\n",
    "\n",
    "**4. depresion.pdf**\n",
    "* Tema: Depresi√≥n\n",
    "* Descripci√≥n: Documento del National Institute of Mental Health. Define la depresi√≥n, sus s√≠ntomas, causas, tipos (mayor, distimia, estacional, posparto, etc.), diagn√≥stico y tratamientos (psicoterapia, medicamentos, terapia de estimulaci√≥n cerebral)\n",
    "\n",
    "\n",
    "**5. Abre-tu-mente-modo-positivo-salud-mental.pdf**\n",
    "* Tema: Promoci√≥n de salud mental en j√≥venes\n",
    "* Descripci√≥n: Gu√≠a educativa dirigida a docentes y familias. Aborda temas como salud mental, adolescencia, estilo de vida saludable, relaciones sociales, emociones, inteligencia emocional y prevenci√≥n de problemas de salud mental.\n",
    "\n",
    "\n",
    "**6. AtencionPrimAmb1988_spa.pdf**\n",
    "* Tema: Atenci√≥n Primaria Ambiental (APA)\n",
    "* Descripci√≥n: Publicaci√≥n de la OPS que presenta el concepto de APA. Ofrece un enfoque hol√≠stico para mejorar el ambiente y la salud p√∫blica. Incluye marco conceptual, principios, problemas ambientales locales, participaci√≥n ciudadana y organizaci√≥n de centros APA\n",
    "\n",
    "\n",
    "**7. 9789243548203_spa.pdf**\n",
    "* Tema: Primera Ayuda Psicol√≥gica (OMS)\n",
    "* Descripci√≥n: Gu√≠a pr√°ctica para trabajadores de campo en situaciones de emergencia. Define PAP, principios de acci√≥n (observar, escuchar, conectar), c√≥mo ayudar de forma responsable y c√≥mo cuidarse como interviniente. Incluye ejercicios y casos simulados\n",
    "\n",
    "\n",
    "**8. empatia.pdf**\n",
    "* Tema: Empat√≠a y neurociencia\n",
    "* Descripci√≥n: Presentaci√≥n sobre empat√≠a desde una perspectiva emocional, cognitiva y neurocient√≠fica.\n",
    "\n",
    "\n",
    "**9. Guiasautoayudadepresionansiedad.pdf**\n",
    "* Tema: Autoayuda para depresi√≥n y ansiedad\n",
    "* Descripci√≥n: Conjunto de gu√≠as del Servicio Andaluz de Salud. Contienen informaci√≥n y actividades sobre depresi√≥n, ansiedad, duelo, autoestima, control de pensamientos, t√©cnicas de relajaci√≥n, afrontamiento del estr√©s, etc√©rtera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "muh29EoTYcDI"
   },
   "source": [
    "## üìÅ Creaci√≥n de Carpeta y Descarga Condicional de Archivos\n",
    "\n",
    "Este bloque de c√≥digo verifica si existe una carpeta llamada `pdf` en el entorno (`/content/pdf`) y realiza las siguientes acciones:\n",
    "\n",
    "* üóÇÔ∏è **Crea la carpeta si no existe**, asegurando el espacio necesario para almacenar los archivos PDF.\n",
    "* üîç **Verifica si la carpeta est√° vac√≠a**:\n",
    "  - Si est√° vac√≠a, **descarga los archivos desde una carpeta p√∫blica en Google Drive** usando `gdown`.\n",
    "  - Si ya contiene archivos, **omite la descarga** para evitar redundancias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qD_iRBEQcHj9",
    "outputId": "fc88c5b0-3ac3-4532-909f-7676a76f5187"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ La carpeta ya existe: /content/pdf\n",
      "‚úÖ La carpeta ya tiene contenidos. Se omite la descarga.\n"
     ]
    }
   ],
   "source": [
    "# Crear la carpeta \"pdf\" si es que no existe\n",
    "folder_path = \"/content/pdf\"\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "    print(f\"‚úÖ Carpeta creada: {folder_path}\")\n",
    "else:\n",
    "    print(f\"üìÅ La carpeta ya existe: {folder_path}\")\n",
    "\n",
    "# Revisar si la carpeta est√° vac√≠a y descargar los archivos de la misma. Si no, saltar descarga.\n",
    "if not os.listdir(folder_path):\n",
    "    print(\"üì• La carpeta est√° vac√≠a. Iniciando descarga de contenidos...\")\n",
    "    folder_url = \"https://drive.google.com/drive/folders/1WxKzp5YHdhS4hf47zFUScL9FHpRSaexE?usp=sharing\"\n",
    "    !gdown --folder {folder_url} -O {folder_path}\n",
    "else:\n",
    "    print(\"‚úÖ La carpeta ya tiene contenidos. Se omite la descarga.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9J9Y_yWYlE3"
   },
   "source": [
    "## üìÑ Extracci√≥n y Visualizaci√≥n de Contenido de Archivos PDF\n",
    "\n",
    "Este bloque realiza la lectura, procesamiento y previsualizaci√≥n del contenido de archivos PDF utilizando PyMuPDF (`fitz`):\n",
    "\n",
    "* **üì• `extract_text_from_pdf(file_path)`**  \n",
    "  Funci√≥n que **abre un archivo PDF y concatena el texto de todas sus p√°ginas** en un solo string, utilizando `fitz` para extraer el texto p√°gina por p√°gina.\n",
    "\n",
    "* **üìÇ Procesamiento de archivos PDF**  \n",
    "  Recorre todos los archivos dentro de la carpeta `/content/pdf`, aplica la funci√≥n anterior y guarda los resultados en un diccionario `pdf_texts` con la estructura `{nombre_archivo: texto_extra√≠do}`.\n",
    "\n",
    "* **üëÄ Previsualizaci√≥n de contenido**  \n",
    "  Por cada archivo procesado, **muestra los primeros 100 caracteres del contenido extra√≠do**, lo cual permite verificar r√°pidamente si la extracci√≥n fue exitosa y si el contenido es legible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sX3Ea3jlghLf",
    "outputId": "a7567672-044c-43c2-b16c-3526ccded3c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Desplegando previzualizaci√≥n del contenido del archivo üìö[Abre-tu-mente-modo-positivo-salud-mental.pdf]:\n",
      "\n",
      "1\n",
      "GU√çA PARA\n",
      "DOCENTES Y FAMILIAS\n",
      "PROMOCI√ìN DE SALUD\n",
      "MENTAL EN J√ìVENES\n",
      "PROMOCI√ìN DE SALUD MENTAL EN J√ì\n",
      "================================================================================\n",
      ">>> Desplegando previzualizaci√≥n del contenido del archivo üìö[Guiasautoayudadepresionansiedad.pdf]:\n",
      "\n",
      "PARA LA DEPRESI√ìN Y LOS TRASTORNOS DE ANSIEDAD\n",
      "PARA LA DEPRESI√ìN Y LOS TRASTORNOS DE ANSIEDAD\n",
      "Servic\n",
      "================================================================================\n",
      ">>> Desplegando previzualizaci√≥n del contenido del archivo üìö[empatia.pdf]:\n",
      "\n",
      "EMPAT√çA\n",
      "EL ARTE DE COMPRENDER EMOCIONES\n",
      "EMPAT√çA\n",
      "EL ARTE DE COMPRENDER EMOCIONES\n",
      "3\n",
      " Mahatma G a n d h\n",
      "================================================================================\n",
      ">>> Desplegando previzualizaci√≥n del contenido del archivo üìö[depresion.pdf]:\n",
      "\n",
      "Depresi√≥n \n",
      " \n",
      "¬øQu√© es la depresi√≥n?\n",
      "Todas las personas se sienten tristes o deca√≠das de vez en cuando\n",
      "================================================================================\n",
      ">>> Desplegando previzualizaci√≥n del contenido del archivo üìö[9789243548203_spa.pdf]:\n",
      "\n",
      "Primera ayuda psicol√≥gica:  Gu√≠a para trabajadores de \n",
      "campo\n",
      "Catalogaci√≥n por la Biblioteca de la OM\n",
      "================================================================================\n",
      ">>> Desplegando previzualizaci√≥n del contenido del archivo üìö[empatia_comunicacion.pdf]:\n",
      "\n",
      "1\n",
      "Empat√≠a en la comunicaci√≥n \n",
      "La empat√≠a en la comunicaci√≥n es fundamental para lograr una conexi√≥n \n",
      "================================================================================\n",
      ">>> Desplegando previzualizaci√≥n del contenido del archivo üìö[Guia de Primeros Auxilios PsicoloÃÅgicos_Integra.pdf]:\n",
      "\n",
      "Gu√≠a de Primeros Auxilios \n",
      "Psicol√≥gicos (PAP) a personas en \n",
      "situaci√≥n de movilidad humana\n",
      "Gu√≠a de P\n",
      "================================================================================\n",
      ">>> Desplegando previzualizaci√≥n del contenido del archivo üìö[primeras-paginas-primeras-paginas-lo-mejor-de-tu-vida-es.pdf]:\n",
      "\n",
      "Lo mejor de tu vida eres t√∫\n",
      "Conf√≠a en la fuerza de tus emociones\n",
      "M.¬™ Jes√∫s √Ålava Reyes\n",
      "√çndice\n",
      "Agrade\n",
      "================================================================================\n",
      ">>> Desplegando previzualizaci√≥n del contenido del archivo üìö[AtencionPrimAmb1988_spa.pdf]:\n",
      "\n",
      "Organizaci√≥n Panamericana de la Salud \n",
      "#k' Organizaci√≥n Mundial de la Salud \n",
      "Divisi√≥n de Salud y Amb\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Funci√≥n para extraer y concatenar el texto de todas las p√°ginas de un archivo PDF usando PyMuPDF (fitz)\n",
    "def extract_text_from_pdf(file_path):\n",
    "    doc = fitz.open(file_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "# Procesar todos los archivos PDF subidos\n",
    "pdf_texts = {}\n",
    "for filename in os.listdir(folder_path):\n",
    "      file_path = os.path.join(folder_path, filename)\n",
    "      text = extract_text_from_pdf(file_path)\n",
    "      pdf_texts[filename] = text\n",
    "\n",
    "# Mostrar los primeros 1000 caracteres de un archivo\n",
    "for name, content in pdf_texts.items():\n",
    "    print(f\">>> Desplegando previzualizaci√≥n del contenido del archivo üìö[{name}]:\\n\")\n",
    "    print(content[:100])\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVO3U5JxBpDi"
   },
   "source": [
    "## üß† Carga de Modelo de Embeddings Multiling√ºe\n",
    "\n",
    "üì¶ A continuaci√≥n se carga un modelo preentrenado de la librer√≠a `sentence-transformers`, espec√≠ficamente el modelo **`paraphrase-multilingual-MiniLM-L12-v2`**, el cual genera embeddings (representaciones vectoriales) de frases o textos.\n",
    "\n",
    "### ‚úÖ Razones para usar este modelo:\n",
    "* üåç **Multiling√ºe:** Soporta m√°s de 50 idiomas, ideal para trabajar con datos en distintos idiomas sin necesidad de traducir.\n",
    "* ‚ö° **Ligero y r√°pido:** Basado en la arquitectura MiniLM, lo que permite un buen equilibrio entre rendimiento y velocidad, incluso sin GPU.\n",
    "* üîç **Precisi√≥n en similitud sem√°ntica:** Entrenado para tareas como detecci√≥n de par√°frasis y b√∫squeda sem√°ntica, lo que lo hace muy √∫til para clustering, recuperaci√≥n de texto y comparaci√≥n de oraciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Bfs5Zxc9j7Uf"
   },
   "outputs": [],
   "source": [
    "# Cargando modelo preentrenado\n",
    "model_sentence = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WqHZsyC_ZmNK"
   },
   "source": [
    "## ‚úÇÔ∏è Divisi√≥n Sem√°ntica de Texto por Oraciones\n",
    "\n",
    "Esta funci√≥n divide un texto en fragmentos **respetando los l√≠mites de tokens y las fronteras sem√°nticas de las oraciones**, usando `sent_tokenize()` de NLTK.\n",
    "\n",
    "### ‚öôÔ∏è ¬øC√≥mo funciona?\n",
    "* üß† Divide el texto en oraciones individuales.\n",
    "* üì¶ Agrupa las oraciones en bloques cuya **longitud no supere `max_tokens` (por defecto 25 palabras)**.\n",
    "* üîÅ Aplica **solapamiento configurable (`overlap`)** entre bloques, preservando coherencia contextual entre fragmentos.\n",
    "* ‚úÖ Devuelve una lista de bloques listos para an√°lisis sem√°ntico, embeddings o respuestas de chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "QRStndlYL_hD"
   },
   "outputs": [],
   "source": [
    "# CHUNKING SEMANTICO CON NLTK\n",
    "# Funci√≥n para dividir el texto en fragmentos (oraciones)\n",
    "def split_text_semantic(text, max_tokens=25, overlap=1):\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_len = 0\n",
    "    for sentence in sentences:\n",
    "        token_count = len(sentence.split())\n",
    "        if current_len + token_count > max_tokens:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = current_chunk[-overlap:] if overlap > 0 else []\n",
    "            current_len = sum(len(s.split()) for s in current_chunk)\n",
    "        current_chunk.append(sentence)\n",
    "        current_len += token_count\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Funci√≥n para dividir el texto en fragmentos (chunks) - NOT USED\n",
    "def split_text(text, chunk_size=500, overlap=50): ##chunk_size=5000, overlap=50)\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), chunk_size - overlap):\n",
    "        chunks.append(text[i:i + chunk_size])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsckZ09AxzBA"
   },
   "source": [
    "## üß† Fragmentaci√≥n, Vectorizaci√≥n y Creaci√≥n del √çndice Sem√°ntico\n",
    "\n",
    "* **Fragmenta textos PDF, genera sus embeddings y construye un √≠ndice FAISS para habilitar b√∫squedas sem√°nticas en un chatbot.**\n",
    "\n",
    "### ‚öôÔ∏è ¬øC√≥mo funciona?\n",
    "\n",
    "* üß© **Fragmentaci√≥n sem√°ntica de documentos**  \n",
    "  Recorre los textos extra√≠dos de los PDFs y los divide en fragmentos utilizando `split_text_semantic`, que respeta los l√≠mites de oraci√≥n y evita cortes arbitrarios. Luego, cada fragmento se guarda junto con su metadato correspondiente (nombre del archivo y contenido del fragmento).\n",
    "\n",
    "* üß¨ **Generaci√≥n y normalizaci√≥n de embeddings**  \n",
    "  Los fragmentos generados se convierten en vectores sem√°nticos (embeddings) mediante un modelo de `sentence-transformers`. A continuaci√≥n, se normalizan para que todos tengan norma unitaria, lo cual es necesario para aplicar similitud coseno correctamente.\n",
    "\n",
    "* üß≠ **Construcci√≥n del √≠ndice FAISS**  \n",
    "  Se crea un √≠ndice FAISS utilizando `IndexFlatIP`, dise√±ado para trabajar con similitud coseno entre vectores normalizados. Luego se agregan todos los embeddings al √≠ndice para habilitar b√∫squedas r√°pidas y precisas.\n",
    "\n",
    "Este flujo pemrite que el chatbot pueda realizar b√∫squedas sem√°nticas inteligentes y responder preguntas con base en el contenido de m√∫ltiples documentos PDF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "6fd507c5b2614dfe9ab5f46e7af21ea2",
      "e537b3a2ffb94320b1d5646431f2e296",
      "84fff814381f4a3f9a189e720f70cd7f",
      "33c8f71406ac44699d90e673daade9ea",
      "301f33fd321c422ab06cfacbdcefd5c2",
      "b66fbd196cd9469d933505d6e30b50d8",
      "d2cf837f9ce948bd8647567cfd8b3871",
      "c2959acc3c464c9ab1386abca210d317",
      "e8c7bdf2dc7e45bcbda5207ef12149c6",
      "2002449ee5604a848686f34eb429da43",
      "1b11ce4f23494161a718070f764ea8be"
     ]
    },
    "id": "CikjqV0MiO0L",
    "outputId": "de5ea7ce-8596-486a-ec38-9720a063e742"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fd507c5b2614dfe9ab5f46e7af21ea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Recorrer todos los archivos PDF, divide su contenido en fragmentos,\n",
    "# y guarda tanto los textos como los metadatos (nombre del archivo y texto del fragmento)\n",
    "all_chunks = []\n",
    "metadata = []\n",
    "for filename, text in pdf_texts.items():\n",
    "    #chunks = split_text(text)\n",
    "    chunks = split_text_semantic(text)\n",
    "    all_chunks.extend(chunks)\n",
    "    metadata.extend([{\"source\": filename, \"text\": chunk} for chunk in chunks])\n",
    "\n",
    "# Crear embeddings\n",
    "embeddings = model_sentence.encode(all_chunks, show_progress_bar=True)\n",
    "embeddings = normalize(embeddings, axis=1)  # normalizar para usar cosine similarity (IndexFlatIP)\n",
    "\n",
    "# Crear √≠ndice FAISS\n",
    "dimension = embeddings.shape[1]\n",
    "#index = faiss.IndexFlatL2(dimension)\n",
    "index = faiss.IndexFlatIP(dimension) # Se usa IndexFlatIP oara cosine similatiry\n",
    "index.add(np.array(embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLr68Bo-yI9T"
   },
   "source": [
    "## üíæ Guardado del √çndice y Metadatos\n",
    "\n",
    "* **Guarda el √≠ndice FAISS y los metadatos para reutilizarlos sin reprocesar los documentos.**\n",
    "\n",
    "### ‚öôÔ∏è ¬øC√≥mo funciona?\n",
    "\n",
    "* üß≠ **Guardar el √≠ndice FAISS**  \n",
    "  Se almacena el √≠ndice generado en un archivo `.faiss`, lo que permite reutilizarlo en futuras sesiones sin tener que volver a generar todos los embeddings y reconstruir el √≠ndice desde cero.\n",
    "\n",
    "* üóÇÔ∏è **Guardar los metadatos**  \n",
    "  Los metadatos (informaci√≥n del fragmento y su documento de origen) se guardan en un archivo `.pkl` usando `pickle`. Esto es esencial para que, al recuperar un fragmento desde el √≠ndice, el chatbot pueda identificar de qu√© archivo proviene y mostrarlo correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "t4fkfNKljEhv"
   },
   "outputs": [],
   "source": [
    "# Guardar FAISS index\n",
    "faiss.write_index(index, \"chatbot_index.faiss\")\n",
    "\n",
    "# Guardar los metadatos\n",
    "with open(\"chatbot_metadata.pkl\", \"wb\") as f:\n",
    "    pickle.dump(metadata, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KF87r29u5glB"
   },
   "source": [
    "# **Sistema RAG + LLM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4eLAz39byWFB"
   },
   "source": [
    "Un sistema **RAG** (Retrieval-Augmented Generation) es una arquitectura de inteligencia artificial que combina recuperaci√≥n de informaci√≥n con generaci√≥n de texto, con el objetivo de mejorar la precisi√≥n, relevancia y actualidad de las respuestas generadas por modelos de lenguaje."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqrMSMxG42ZJ"
   },
   "source": [
    "## üß† Modelos LLM y de Embeddings\n",
    "\n",
    "* **Carga un modelo de lenguaje para generaci√≥n de respuestas y un modelo de embeddings para comprensi√≥n sem√°ntica.**\n",
    "\n",
    "### ‚öôÔ∏è ¬øC√≥mo funciona?\n",
    "\n",
    "* üí¨ **Modelo generador de respuestas (LLM)**  \n",
    "  Se carga el modelo `google/flan-t5-base`, un modelo de tipo encoder-decoder optimizado para tareas text-to-text.  \n",
    "  A trav√©s de `AutoTokenizer` y `AutoModelForSeq2SeqLM` de `transformers`, se inicializa el modelo junto con su tokenizador, y se configura un `pipeline` de generaci√≥n de texto (`text2text-generation`).  \n",
    "  Este modelo permite al chatbot **redactar respuestas completas en lenguaje natural** a partir de consultas y contextos.\n",
    "\n",
    "* üß© **Modelo de embeddings sem√°nticos**  \n",
    "  Se carga el modelo `paraphrase-multilingual-MiniLM-L12-v2` de `sentence-transformers`, el cual convierte oraciones o fragmentos de texto en vectores num√©ricos.  \n",
    "  Estos vectores permiten comparar significados y buscar respuestas relevantes dentro del √≠ndice FAISS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SsTf0kyInanM",
    "outputId": "a7caba21-cfd2-4187-83a6-bce771493613"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# LLM para generar respuestas\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Modelo para embeddings (vectorizaci√≥n)\n",
    "embedder = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qEm3RbcU5Je0"
   },
   "source": [
    "## üß† B√∫squeda Sem√°ntica y Generaci√≥n de Respuestas\n",
    "\n",
    "* **Estas funciones permiten buscar fragmentos relevantes mediante similitud sem√°ntica y generar respuestas con un modelo LLM.**\n",
    "\n",
    "### ‚öôÔ∏è ¬øC√≥mo funciona?\n",
    "\n",
    "* üîé **`search_similar_chunks( )`**  \n",
    "  Genera un embedding para la consulta del usuario y lo compara contra el √≠ndice FAISS para encontrar los fragmentos m√°s relevantes.  \n",
    "  Si no se encuentra ning√∫n resultado con una similitud suficiente (basada en el `threshold`), retorna `None`.  \n",
    "  Si hay coincidencias, devuelve los textos correspondientes desde los metadatos.\n",
    "\n",
    "* üß© **`build_prompt( )`**  \n",
    "  Construye un mensaje (prompt) combinando los fragmentos de contexto recuperados con la pregunta del usuario.  \n",
    "  Este prompt es lo que se le entrega al modelo generador para que forme una respuesta informada y contextualizada.\n",
    "\n",
    "* üí¨ **`generate_answer( )`**  \n",
    "  Orquesta el flujo completo: busca los fragmentos m√°s parecidos a la consulta, genera el prompt y produce una respuesta con el modelo `flan-t5-base`.  \n",
    "  Si no se encuentran fragmentos relevantes, devuelve un mensaje de disculpa.\n",
    "\n",
    "* üíæ **Carga del √≠ndice y metadatos**  \n",
    "  Se cargan el √≠ndice FAISS (`chatbot_index.faiss`) y los metadatos (`chatbot_metadata.pkl`) previamente guardados, para que el sistema est√© listo para responder sin tener que reentrenar o reprocesar documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "WlVqMthHneso"
   },
   "outputs": [],
   "source": [
    "def build_prompt(query, context_chunks):\n",
    "    context = \"\\n\".join(context_chunks)\n",
    "    prompt = (\n",
    "        f\"Con este contexto:\\n\"\n",
    "        f\"{context}\\n\\n\"\n",
    "        f\"Responde brevemente:\\n\"\n",
    "        f\"{query}\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def search_similar_chunks(query, top_k=3, threshold=0.3):\n",
    "    query_embedding = embedder.encode([query])\n",
    "    query_embedding = normalize(query_embedding, axis=1) #Normalizar los embeddings\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    if max(distances[0] < threshold):\n",
    "      return None\n",
    "\n",
    "    results = []\n",
    "    for i in indices[0]:\n",
    "        results.append(metadata[i]['text'])\n",
    "    return results\n",
    "\n",
    "def generate_answer(query, top_k=3):\n",
    "    chunks = search_similar_chunks(query, top_k=top_k)\n",
    "    if chunks is None:\n",
    "      return \"Lo siento, no encontr√© informaci√≥n suficiente para responder a eso.\"\n",
    "    prompt = build_prompt(query, chunks)\n",
    "\n",
    "    output = generator(prompt, max_new_tokens=256, do_sample=False)\n",
    "    return output[0]['generated_text']\n",
    "\n",
    "# Cargar √≠ndice\n",
    "index = faiss.read_index(\"chatbot_index.faiss\")\n",
    "\n",
    "# Cargar metadatos\n",
    "with open(\"chatbot_metadata.pkl\", \"rb\") as f:\n",
    "    metadata = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOfeqsQb5ttD"
   },
   "source": [
    "# **El chatbot, incluyendo ejemplos de prueba.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OiyJ8qcM5ltH"
   },
   "source": [
    "## ü§ñ Prueba de Consultas al Chatbot\n",
    "\n",
    "* **Eval√∫a una lista de preguntas simuladas y muestra las respuestas generadas.**\n",
    "\n",
    "### ‚öôÔ∏è ¬øC√≥mo funciona?\n",
    "\n",
    "* üßæ **Lista de preguntas (`queries`)**  \n",
    "  Contiene ejemplos representativos de preguntas que un usuario podr√≠a hacerle al chatbot, relacionadas con ansiedad, depresi√≥n y emociones humanas.\n",
    "\n",
    "* üîÑ **Bucle de interacci√≥n con el chatbot**  \n",
    "  Para cada pregunta:\n",
    "  - Se imprime la consulta.\n",
    "  - Se llama a `generate_answer()` para obtener una respuesta contextual.\n",
    "  - La respuesta se imprime.\n",
    "\n",
    "Este bloque permite probar y validar c√≥mo responde el sistema a distintas consultas, ideal para demostraciones, testing o debugging del chatbot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0t3eNHkfkGvx",
    "outputId": "736fb80b-06cc-4124-87a0-750ac401bca7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      ">> üë®üèª‚Äçüíª Pregunta realizada:\n",
      "C√≥mo superar la ansiedad?\n",
      "\n",
      "++ üíª Respuesta del chatbot:\n",
      "‚Ä¢ Controle su preocupaci√≥n. Escoja un lugar y una hora para preocuparse. ‚Ä¢ Los sntomas de ansiedad originales pueden volver. Considere otras formas de hacer frente a sus sntomas. For example, aprenda a relajarse. Cambiar los comportamientos relacionados con la ansiedad ‚Ä¢ Trate de reconocer cuando est√° evitando cosas. ‚Ä¢ Siempre que se puede tratar de hacer frente a estostemores, no todos a la vez pero siempre gradual. Responde briefly:\n",
      "============================================================\n",
      ">> üë®üèª‚Äçüíª Pregunta realizada:\n",
      "Qu√© es ansiedad?\n",
      "\n",
      "++ üíª Respuesta del chatbot:\n",
      "Para la ansiedad Los sntomas de la ansiedad incluyen: agitaci√≥n, tensi√≥n, irritabilidad, palpitaciones, temblores, sudoraci√≥n, preocuparse en exceso, dormir mal, falta de concentraci√≥n, respiraci√≥n rapida, sensaci√≥n de ‚Äúpellizco en el est√≥mago‚Äù y tensi√≥n muscular\n",
      "============================================================\n",
      ">> üë®üèª‚Äçüíª Pregunta realizada:\n",
      "Cuando es grave la ansiedad?\n",
      "\n",
      "++ üíª Respuesta del chatbot:\n",
      "‚Ä¢ Graves and desagradables. ‚Ä¢ Duran mucho tiempo. ‚Ä¢ Ocurren con demasiada frecuencia. S. Son los siguientes: ‚Ä¢ Trastorno de ansiedad generalizada. La ansiedad puede ser una sensaci√≥n general y constante de preocupaci√≥n. Para la ansiedad Los sntomas de la ansiedad incluyen: agitaci√≥n, tensi√≥n, irritabilidad, palpitaciones, temblores, sudoraci√≥n, preocuparse en exceso, dormir mal, falta de concentraci√≥n, respiraci√≥n rapida, sensaci√≥n de ‚Äúpellizco en el est√≥mago‚Äù y tensi√≥n muscular\n",
      "============================================================\n",
      ">> üë®üèª‚Äçüíª Pregunta realizada:\n",
      "Cuales son los sintomas de la ansiedad\n",
      "\n",
      "++ üíª Respuesta del chatbot:\n",
      "agitaci√≥n, tensi√≥n, irritabilidad, palpitaciones, temblores, sudoraci√≥n, preocuparse en exceso, dormir mal, falta de concentraci√≥n, respiraci√≥n r√°pida, sensaci√≥n de ‚Äúpellizco en el est√≥mago‚Äù y tensi√≥n muscular.\n",
      "============================================================\n",
      ">> üë®üèª‚Äçüíª Pregunta realizada:\n",
      "Qu√© hacer si estoy triste?\n",
      "\n",
      "++ üíª Respuesta del chatbot:\n",
      "Me echo a llorar por cualquier cosa y todo me sienta mal. No digas ‚ÄúEstoy triste‚Äù si las palabras que mejor describiran tu estado emocional seran decepcionado, compungido, melanc√≥lico o herido. S√© concreto.\n",
      "============================================================\n",
      ">> üë®üèª‚Äçüíª Pregunta realizada:\n",
      "Qu√© es una depresi√≥n?\n",
      "\n",
      "++ üíª Respuesta del chatbot:\n",
      "La depresi√≥n es una enfermedad, como lo es la diabetes o una lcera de est√≥mago\n",
      "============================================================\n",
      ">> üë®üèª‚Äçüíª Pregunta realizada:\n",
      "¬øC√∫ales son las causas de la depresi√≥n?\n",
      "\n",
      "++ üíª Respuesta del chatbot:\n",
      "Para lo general, se considera que existen ‚Äúfactores‚Äù biol√≥gicos, psicol√≥gicos y ambientales (aqu√©ls relacionados con el entorno social o la familia).\n",
      "============================================================\n",
      ">> üë®üèª‚Äçüíª Pregunta realizada:\n",
      "Quieres ser mi amigo?\n",
      "\n",
      "++ üíª Respuesta del chatbot:\n",
      "Puedo encontrar a una amiga que me acompae y aprovechar para conversar.\n",
      "============================================================\n",
      ">> üë®üèª‚Äçüíª Pregunta realizada:\n",
      "Que es un iPhone?\n",
      "\n",
      "++ üíª Respuesta del chatbot:\n",
      "Lo siento, no encontr√© informaci√≥n suficiente para responder a eso.\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"C√≥mo superar la ansiedad?\",\n",
    "    \"Qu√© es ansiedad?\",\n",
    "    \"Cuando es grave la ansiedad?\",\n",
    "    \"Cuales son los sintomas de la ansiedad\",\n",
    "    \"Qu√© hacer si estoy triste?\",\n",
    "    \"Qu√© es una depresi√≥n?\",\n",
    "    \"¬øC√∫ales son las causas de la depresi√≥n?\",\n",
    "    \"Quieres ser mi amigo?\",\n",
    "    \"Que es un iPhone?\"\n",
    "           ]\n",
    "\n",
    "for query in queries:\n",
    "  print(f\"=\"*60)\n",
    "  print(f\">> üë®üèª‚Äçüíª Pregunta realizada:\\n{query}\\n\")\n",
    "  respuesta = generate_answer(query)\n",
    "  print(\"++ üíª Respuesta del chatbot:\")\n",
    "  print(respuesta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBUXG3vr56ev"
   },
   "source": [
    "## üñºÔ∏è Despliegue de Interfaz Interactiva con Gradio\n",
    "\n",
    "* **Por √∫ltimo, se crea una interfaz web simple para interactuar con el chatbot directamente desde el navegador.**\n",
    "\n",
    "### ‚öôÔ∏è ¬øC√≥mo funciona?\n",
    "\n",
    "* üîå **`chat_interface(question)`**  \n",
    "  Define una funci√≥n que toma una pregunta como entrada, genera una respuesta utilizando `generate_answer()`, y la retorna. Esta ser√° la funci√≥n que conecta la interfaz con la l√≥gica del chatbot.\n",
    "\n",
    "* üß™ **`gr.Interface(...)`**  \n",
    "  Utiliza la librer√≠a `Gradio` para crear una interfaz gr√°fica m√≠nima:\n",
    "  - `inputs=\"text\"` permite al usuario ingresar una pregunta en una caja de texto.\n",
    "  - `outputs=\"text\"` muestra la respuesta generada.\n",
    "  - `launch(share=True)` despliega la interfaz y genera un enlace p√∫blico para compartirla f√°cilmente.\n",
    "\n",
    "  ### Captura de interfaz con Gradio:\n",
    "  <img src=\"https://drive.google.com/uc?id=1cyyi2xpzoqWzWiK4OQVcgSx9eUTEwXwx\" width=\"1200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6QVsp7TrQsvS"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "id": "Lw1bxsYeQrMn",
    "outputId": "7de3cd47-5e50-446b-de15-cb2ffe70d3af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://7b1c0885a6dfdd66d2.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://7b1c0885a6dfdd66d2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "def chat_interface(question):\n",
    "    answer = generate_answer(question)\n",
    "    return answer\n",
    "gr.Interface(fn=chat_interface, inputs=\"text\", outputs=\"text\").launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kx-dZSFJz9cK"
   },
   "source": [
    "\n",
    "# **Conclusiones:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3w3usdaC0BCj"
   },
   "source": [
    "* #### **Conclusiones de la actividad chatbot LLM + RAG:**\n",
    "\n",
    "**Aprendizajes generales**\n",
    "- Como equipo, logramos implementar un enfoque de Retrieval-Augmented Generation que permiti√≥ combinar la potencia de los modelos generativos con la precisi√≥n del sistema de recuperaci√≥n documental mediante FAISS. Esto demostr√≥ ser √∫til para responder preguntas espec√≠ficas del dominio de salud mental, generando respuestas contextualizadas y coherentes.\n",
    "\n",
    "- Al utilizar corpus en espa√±ol y modelos preentrenados compatibles con el idioma, se busc√≥ la adecuaci√≥n ling√º√≠stica del chatbot para usuarios hispanohablantes. Este enfoque nos confirma la importancia de adaptar los modelos LLM a las necesidades culturales y ling√º√≠sticas de los usuarios.\n",
    "\n",
    "- El prototipo desarrollado tiene potencial para convertirse en una herramienta de apoyo accesible para personas en b√∫squeda de informaci√≥n confiable y actualizada relacionada con salud mental.\n",
    "\n",
    "- Se comprendi√≥ la arquitectura RAG como una soluci√≥n que combina lo mejor del acceso a informaci√≥n (retrieval) con la generaci√≥n fluida de lenguaje natural (LLM).\n",
    "\n",
    "- Se identific√≥ que los modelos LLM por s√≠ solos pueden llegar a tener l√≠mites si no est√°n conectados a una base de conocimiento actualizada o contextual.\n",
    "\n",
    "- El enfoque RAG permiti√≥ ofrecer respuestas m√°s precisas, relevantes y trazables, especialmente √∫til en temas delicados como salud mental.\n",
    "\n",
    "\n",
    "**T√©cnicas y Herramientas Implementadas**\n",
    "\n",
    "- Se utilizaron embeddings sem√°nticos para representar texto y permitir b√∫squedas por similitud en lugar de coincidencia exacta de palabras clave.\n",
    "\n",
    "- Se trabaj√≥ con herramientas como FAISS para construir √≠ndices de recuperaci√≥n eficientes.\n",
    "\n",
    "- Se aplic√≥ un umbral de similitud para filtrar respuestas no relacionadas, mejorando la precisi√≥n del chatbot.\n",
    "\n",
    "- Se integr√≥ el sistema en una interfaz interactiva mediante Gradio, facilitando la interacci√≥n con el usuario final.\n",
    "\n",
    "- Se reafirm√≥ la importancia del preprocesamiento del texto, incluyendo limpieza, normalizaci√≥n y tokenizaci√≥n.\n",
    "\n",
    "- Se emplearon modelos preentrenados de HuggingFace Transformers para tareas de generaci√≥n de texto, en busca de coherencia y naturalidad.\n",
    "\n",
    "- Se comprob√≥ que los LLM pueden ampliar, matizar o contextualizar mejor la informaci√≥n cuando se utilizan junto con RAG.\n",
    "\n",
    "- Se logr√≥ generar respuestas basadas en fuentes espec√≠ficas.\n",
    "\n",
    "- Se apreciaron los beneficos de que es un enfoque modular y escalable, permitiendo incorporar nuevos documentos sin necesidad de reentrenar el modelo.\n",
    "\n",
    "**Retos y Reflexiones**\n",
    "\n",
    "- Se observan oportunidades en las respuestas, ya que hay errores en cuanto a formato, mezcla de idiomas e incluso omisi√≥n de caracteres. Esto pudiera mejorarse ya sea agregando instrucciones al promppt, hacer un proceso de limpieza m√°s extensivo o aplicar filtros de idioma.\n",
    "\n",
    "- Se identific√≥ que definir un umbral de similitud adecuado es clave para evitar respuestas irrelevantes.\n",
    "\n",
    "- Se observ√≥ que la curaci√≥n del corpus (selecci√≥n y fragmentaci√≥n del contenido) tiene un impacto directo en la calidad del chatbot.\n",
    "\n",
    "- Se concluy√≥ que este enfoque puede adaptarse a otros dominios como educaci√≥n, derecho, atenci√≥n m√©dica o servicio al cliente.\n",
    "\n",
    "- Se reconoci√≥ que la estructura RAG es multiling√ºe y adaptable, facilitando su implementaci√≥n en diversos contextos.\n",
    "\n",
    "- Se evidenci√≥ el potencial para escalar esta tecnolog√≠a hacia sistemas m√°s complejos como tutores virtuales, asistentes personales o motores de b√∫squeda especializados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CtB5Q3m41YQ0"
   },
   "source": [
    "# **Fin de la actividad chatbot: LLM + RAG**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
